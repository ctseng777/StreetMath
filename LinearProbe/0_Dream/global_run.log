nohup: ignoring input
==================================================
Starting: linear_probe_dream_near_2.py
Time: Wed Jan 21 00:41:40 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-2
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:35, 11.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:24<00:24, 12.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  7.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.13s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 23: acc=0.990
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 27: acc=0.566
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_2.json

Finished: linear_probe_dream_near_2.py
Logs saved to: linear_probe_dream_near_2.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_3.py
Time: Wed Jan 21 00:44:43 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-3
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:24, 12.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.99s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 20: acc=0.718
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 1: acc=0.640
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_3.json

Finished: linear_probe_dream_near_3.py
Logs saved to: linear_probe_dream_near_3.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_4.py
Time: Wed Jan 21 00:47:58 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-4
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:23, 11.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:34<00:11, 11.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.85s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 8: acc=0.884
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 2: acc=0.730
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_4.json

Finished: linear_probe_dream_near_4.py
Logs saved to: linear_probe_dream_near_4.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_6.py
Time: Wed Jan 21 00:51:14 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-6
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:24, 12.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.98s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 28: acc=0.626
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 15: acc=0.542
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_6.json

Finished: linear_probe_dream_near_6.py
Logs saved to: linear_probe_dream_near_6.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_7.py
Time: Wed Jan 21 00:54:25 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-7
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream model: Dream-org/Dream-v0-Instruct-7B...
Warning: Error loading with AutoModelForCausalLM: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Attempting fallback to AutoModel...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:24, 12.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:34<00:11, 11.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.89s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Warning: Error getting layer meta: Expected attn_mask dtype to be bool or float or to match query dtype, but got attn_mask.dtype: long int and  query.dtype: c10::Half instead.
Layers: 29 | Width: 3584
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 6: acc=0.606
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 2: acc=0.584
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_7.json

Finished: linear_probe_dream_near_7.py
Logs saved to: linear_probe_dream_near_7.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_8.py
Time: Wed Jan 21 00:57:36 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-8
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:24<00:24, 12.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.96s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 9: acc=0.652
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 20: acc=0.658
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_8.json

Finished: linear_probe_dream_near_8.py
Logs saved to: linear_probe_dream_near_8.log
Cleaning up...
==================================================
Starting: linear_probe_dream_near_9.py
Time: Wed Jan 21 01:00:52 UTC 2026
==================================================
`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
Linear Probing on Dream-v0-Instruct-7B — Target: Near-9
============================================================

========== DREAM: Dream-org/Dream-v0-Instruct-7B ==========
Loading Dream: Dream-org/Dream-v0-Instruct-7B
Fallback due to: Unrecognized configuration class <class 'transformers_modules.Dream_hyphen_org.Dream_hyphen_v0_hyphen_Instruct_hyphen_7B.05334cb9faaf763692dcf9d8737c642be2b2a6ae.configuration_dream.DreamConfig'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:24<00:24, 12.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  7.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.02s/it]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
  [Progress] Starting stream for 2000 samples (125 batches)...
    Processing batch 10/125...
    Processing batch 20/125...
    Processing batch 30/125...
    Processing batch 40/125...
    Processing batch 50/125...
    Processing batch 60/125...
    Processing batch 70/125...
    Processing batch 80/125...
    Processing batch 90/125...
    Processing batch 100/125...
    Processing batch 110/125...
    Processing batch 120/125...
Evaluating digits_paraphrase...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 5: acc=0.672
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Evaluating words...
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
  Best layer 5: acc=0.700
  [Progress] Starting stream for 500 samples (32 batches)...
    Processing batch 10/32...
    Processing batch 20/32...
    Processing batch 30/32...
Done. Results saved to probe_results_dream_near_9.json

Finished: linear_probe_dream_near_9.py
Logs saved to: linear_probe_dream_near_9.log
Cleaning up...
==================================================
All Dream_7B experiments completed!
